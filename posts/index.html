<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Posts &middot; Rows and Columns</title>

    <meta name="description" content="Journal of a Database Admin">

    <meta name="generator" content="Hugo 0.78.2" />
    <meta name="twitter:card" content="summary">
    
    <meta name="twitter:title" content="Posts &middot; Rows and Columns">
    <meta name="twitter:description" content="Journal of a Database Admin">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Posts &middot; Rows and Columns">
    <meta property="og:description" content="Journal of a Database Admin">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

    <link rel="stylesheet" href='https://jlucasdba.github.io/css/all.min.css'>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="Rows and Columns" href='https://jlucasdba.github.io/index.xml' />
</head>
<body>


<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="https://jlucasdba.github.io/">Rows and Columns</a></h1>
            <h2 class="brand-tagline"> Journal of a Database Admin </h2>
            <h2><a href="/about">About Me</a></h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                
                
                
                <li class="nav-item">
                    <a class="pure-button" href='https://jlucasdba.github.io/index.xml'>
                        <i class="fa fa-rss"></i> rss
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">31 Dec 2020, 13:51</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://jlucasdba.github.io/posts/shared_buffers/" class="post-title">Postgres shared_buffers</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Let&rsquo;s talk about shared_buffers in postgres. Postgres has very good documentation, but the tuning of this important parameter is one area where the documentation is unfortunately a bit vague.</p>
<p>When postgres starts up, it grabs a chunk of memory that is shared between all backend server processes. Shared_buffers controls the size of this shared memory pool. Every page the database reads or writes passes through this shared memory.</p>
<p>Everytime the database reads a page from disk, it&rsquo;s stored in these shared buffers. Depending on factors we&rsquo;ll go into later, it may be cached in memory for a longer or shorter period of time, but it will always be stored at least briefly. Cacheing is important because even fast SSDs are slower than RAM. The less often the database has to perform disk access, the better its performance will be.</p>
<p>Similarly, everytime the database writes a page, it&rsquo;s stored in the shared buffers. Writes are written to WAL immediately, but writes to table datafiles are held in memory so that disk io can (ideally) be spread out over time. This also has the added benefit that this recently written page is available in memory if it needs to be read again quickly.</p>
<p>At this point, you may think, &ldquo;Great, I&rsquo;ll just make shared_buffers the same size as my RAM, give all the memory to postgres to manage, and call it a day.&rdquo; Well, not so fast. For one thing, the operating system and any other processes on the system need memory to operate too. For another, postgres backend processes use per-process, non-shared memory as well. Operations like in-memory sorts are done in memory allocated to the individual backend processes, not the shared buffers. So there also needs to be memory available for these processes.</p>
<p>The other thing to consider is that postgres is designed to rely on the filesystem cache provided by the operating system. You&rsquo;ll see this mentioned in various sources online, but here&rsquo;s what it means in practice. Scans of large tables do not cache their pages in the shared buffers. Any scan larger than 25% of the shared_buffers size instead allocates a small ring buffer in the buffer pool, reads pages into that, and immediately cycles them out as new pages are loaded in. This is to avoid a single large scan evicting everything else from the shared buffers. This does mean that the pages from those large scans don&rsquo;t remain cached. But, postgres is counting on the fact that those large table pages are (hopefully) also being cached by the operating system, so repeated reads should still avoid expensive disk accesses. The more memory you allocate to shared_buffers, the less memory the operating system has to cache files.</p>
<p>Similarly, bulk write operations like COPY or CREATE TABLE AS SELECT also use a somewhat larger ring buffer for their page writes to avoid evicting the entire buffer cache.</p>
<p>There&rsquo;s one final, related factor to consider, which is the double-cacheing effect. We&rsquo;ve established that both postgres and the operating system are cacheing pages. In many cases they will both be cacheing the <strong>same</strong> pages. To some extent this is unavoidable, as maintaining data in shared buffers is necessary. But the larger your shared_buffers area is, the more memory capacity you are likely to be wasting by cacheing data at both the postgres and operating system level.</p>
<p>Most recommendations you&rsquo;ll see online are that a good starting point for shared_buffers on a dedicated database server is 25% of your RAM. For some workloads it may be advantageous to go as high as 40%, but probably not more than about 8GiB.</p>
<p>This is good starting advice, but let&rsquo;s try to dig a little deeper.</p>
<p>Cases for having a larger shared_buffers:</p>
<ul>
<li>If your workload is write heavy, more shared_buffers is probably helpful. You have more room to cache writes in memory before they have to be flushed to disk.</li>
<li>If you&rsquo;ve increased max_wal_size and/or checkpoint_timeout, you probably also want to increase shared_buffers. Increasing these settings means your checkpoints are farther apart. That means (assuming the same level of traffic) more writes between checkpoints, so it&rsquo;s advantageous to also have more memory to buffer them in.</li>
<li>If you have a lot of small tables, or at least a small subset of active data that you mostly do indexed lookups on, you may want to increase shared_buffers. The best case scenario is that all of your frequently read data can be fully cached in shared buffers, and you never need to read from disk at all once the cache is populated. Can be difficult in practice though, unless your working set is small, or you have a whole lot of memory.</li>
</ul>
<p>Cases for having a smaller shared_buffers:</p>
<ul>
<li>If you have a lot of large tables that you scan frequently, you probably want a lower shared_buffers. This may seem counterintuitive, but the pages read during these large scans won&rsquo;t be cached by postgres anyway, so better to leave as much room as possible for the operating system cache.</li>
<li>If you do a lot of bulk load operations, these are also processed using a ring buffer and pushed immediately to storage, so a large shared_buffers setting won&rsquo;t help here either.</li>
</ul>
<p>So based on these points, write heavy OLTP workloads with a relatively small active dataset will probably benefit from a larger setting for shared_buffers. And OLAP workloads with a lot of bulk writes and large table scans won&rsquo;t benefit much, and should use a smaller shared_buffers setting to avoid wasting memory.</p>
<p>Of course the tricky part is that many database workloads don&rsquo;t fall neatly into one of these two categories. In that case you have to compromise. The only way to know for sure is going to be to test how postgres behaves under your specific workload. Hopefully understanding these fundamentals will at least help you make more informed tuning choices.</p>
<p>In any case, the more RAM you have available on your database server the better. Even if you don&rsquo;t allocate it explicitly to shared_buffers, it can still be used for operating system cacheing, and that will have benefits as well.</p>
<p>Special shoutout to Hironobu Suzuki&rsquo;s online book <a href="http://www.interdb.jp/pg/index.html">The Internals of PostgreSQL</a>, which went a long way towards helping me understand postgres&rsquo;s memory management. Recommended reading.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">08 Dec 2020, 20:06</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://jlucasdba.github.io/posts/git-filter-branch/" class="post-title">Git filter-branch</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Let&rsquo;s talk about one of the less-understood git commands for a minute: git filter-branch. What do you do if some element exists in your git repository that you need to expunge? Say a password, or a sensitive email address. Well, ideally you should change the password or the email address, because rewriting git history is not something you should undertake lightly. But, if you&rsquo;re a brave soul who ignores the warning signs, and ventures down the dark path, filter-branch is what you use.</p>
<p>If you&rsquo;re at all familiar with git, every commit depends on all the previous gits before it. You can&rsquo;t change an old commit without also rewriting any newer commits. And that&rsquo;s exactly what filter-branch does. It walks through the entire commit history of a branch, applies a filter at each step, and creates new commits from the filtered result.</p>
<p>Here&rsquo;s a really simple example.</p>
<pre><code>* commit c36d131655f4e73ca4101674dde213a0f7a0794c (HEAD, origin/master, origin/HEAD, master)
| Author: James Lucas &lt;sensitive.email&gt;
| Date:   Mon Dec 7 15:16:43 2020 -0600
| 
|     Second commit.
|  
* commit 8fa4b4ae6a4e0abcc2d643988467ef11a91d2844
  Author: James Lucas &lt;sensitive.email&gt;
  Date:   Mon Dec 7 15:16:32 2020 -0600
  
      First commit.
</code></pre><p>Oops. We used an email address we shouldn&rsquo;t have. Maybe a personal email instead of a work email. Filter-branch has a number of different filters. For rewriting metadata info like author email, the one you want is the <code>env-filter</code>. You provide a block to be executed for each processed commit (which might be a complex script in and of itself), and the environment variables set at the end are used to rewrite the commit metadata. (There&rsquo;s a list of available filters and how to use them in the <a href="https://git-scm.com/docs/git-filter-branch">git-filter-branch man page</a>). So in our example, we want to set the GIT_AUTHOR_EMAIL variable, to change the author email to something else.  So our command looks like this:</p>
<pre><code>git filter-branch --env-filter 'export GIT_AUTHOR_EMAIL=&quot;public.email&quot;' master
Rewrite c36d131655f4e73ca4101674dde213a0f7a0794c (2/2)
Ref 'refs/heads/master' was rewritten
</code></pre><p>And the resulting log looks like</p>
<pre><code>* commit 575c0793decabdf517eb611bbd74a6cfeeb14b88 (HEAD, master)
| Author: James Lucas &lt;public.email&gt;
| Date:   Mon Dec 7 15:16:43 2020 -0600
| 
|     Second commit.
|  
* commit af3db6517ddf3437878b7b4ab9dcda13dc5bd427
  Author: James Lucas &lt;public.email&gt;
  Date:   Mon Dec 7 15:16:32 2020 -0600
  
      First commit.
  
* commit c36d131655f4e73ca4101674dde213a0f7a0794c (origin/master, origin/HEAD, refs/original/refs/heads/master)
| Author: James Lucas &lt;sensitive.email&gt;
| Date:   Mon Dec 7 15:16:43 2020 -0600
| 
|     Second commit.
|  
* commit 8fa4b4ae6a4e0abcc2d643988467ef11a91d2844
  Author: James Lucas &lt;sensitive.email&gt;
  Date:   Mon Dec 7 15:16:32 2020 -0600
  
      First commit.
</code></pre><p>Okay, something strange happened here.  We got our new commits, and our master branch is pointing at the rewritten commit history. But there&rsquo;s a parallel commit history in the log as well, with our old commits. This represents two things. The first is a remote branch (origin/master), which we&rsquo;ll get to in a minute. The other is that filter-branch marks your old commit history for you as a backup. This is pretty important, because complex filter situations can be very easy to mess up. This backup history gives you an opportunity to compare the old and new commits, and fallback if something has gone wrong. If you&rsquo;re confident everything is okay, you can remove the backup history with</p>
<pre><code>git update-ref -d refs/original/refs/heads/master
</code></pre><p>Now, let&rsquo;s talk about our remote branch on origin. I mentioned before that filter-branch is dangerous, and here&rsquo;s why. In a local repository, with no connection to anything else, rewriting commits isn&rsquo;t really a big deal. But as soon as you start thinking about distributed operations, rewriting history gets messy. As the log above shows, you&rsquo;ve just rewritten your entire commit history with new commits. If you try to push these to a remote repository (for example, github), the new commits have no common history with your old commits. They represent a completely parallel history. Normally git will reject a push in a situation like this. This is where <code>git push --force</code> (another dangerous command) comes into the picture. You can tell git to ignore the normal safety rules, and overwrite any previous commits in the remote branch. But then any other child repos of your remote will also be out of sync, and need to be re-cloned, or at least carefully cleaned up. Any branches other developers are working on will need to be rebased. And if everyone&rsquo;s not very careful, there&rsquo;s a pretty good chance of developers working on another repository accidentally reintroducing the element you were trying to get rid of in the first place.</p>
<p>In a distributed project, you should absolutely try to avoid rewrites like this if you can help it. Doubly so if the project is public. <strong>Be very careful what you commit, and be even more careful what you push.</strong> But sometimes we all mess up, and git filter-branch is there for the worst of those cases.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">30 Nov 2020, 17:19</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://jlucasdba.github.io/posts/hugo/" class="post-title">Hugo</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>I wanted to do a quick post about the blogging setup I&rsquo;m using, and why I chose it. The purpose of this blog is to provide a place to do write-ups on topics of interest I run across in my work, and share them with the community. Github Pages seemed like a natural (and inexpensive!) hosting solution, given the target audience.</p>
<p>While I was researching hosting, the topic of static site generators came up. These tools are essentially static website compilers. You build your site out of component pieces (templates, static assets, markdown), and feed it into the site generator to generate a fully realized html site. The advantage of a workflow like this is it provides a lot of flexibility with fairly minimal setup and maintenance. You don&rsquo;t need a database, and adding a new post is as simple as adding a new markdown file and rebuilding the site.</p>
<p>The site generator I&rsquo;m using is called <a href="https://gohugo.io">Hugo</a>. It&rsquo;s a tool with a special emphasis on creating hierarchical pages of posted content - ideal for a blog. There&rsquo;s a large number of community-built themes available, and it&rsquo;s easy to override individual theme elements to customize for your needs. Deployment is just pushing a new compiled version of the site into a git repo.</p>
<p>There&rsquo;s a bit of a learning curve to be sure, but once it&rsquo;s up and running the maintenance is near zero. Definitely an option worth looking at, particularly for a blog or Github project site.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">25 Nov 2020, 11:29</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://jlucasdba.github.io/posts/tablebloat/" class="post-title">Postgres Table Bloat</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>One of the things DBAs coming from other database systems (like Oracle) need to be aware of when coming to Postgres is how Postgres manages changed data. This is covered in depth in a lot of other places, so I won&rsquo;t repeat in too much detail. The short version is that when a row is updated or deleted, Postgres maintains the old version of that row within the table data file. Visibility of the old version to concurrent transactions is controlled by transaction id. Eventually, when the old row versions are no longer needed, they are garbage collected by the table vacuum process.</p>
<p>This is different than a system like Oracle, where old data is written to a separate space (undo), referenced when needed, and discarded when no longer needed. Both approaches have positives and negatives. Postgres' advantage is that transaction rollbacks are fast. The data is still in the datafiles, so a rollback is simply a question of manipulating transaction ids. In an undo-based system, rollbacks can take a very long time if a lot of data has to be copied back out of undo. The disadvantage is that heavily written tables in Postgres can become physically very large, even if the number of live rows is relatively small. This is often referred to as &ldquo;table bloat&rdquo;. Vacuuming more frequently can help, but may not eliminate the problem. It&rsquo;s possible to wind up with large empty spaces in undo-based systems, but this really only happens as a result of large-scale deletes, and the effect is not really cumulative like it can be in Postgres.</p>
<p>We can illustrate this with a simple example. Note that I&rsquo;ve disabled autovacuum for this example. Please don&rsquo;t do that in live systems.</p>
<p>First we setup some standard extensions we&rsquo;re going to use for this example:</p>
<pre><code>postgres=# create extension pgstattuple;
CREATE EXTENSION
postgres=# create extension pageinspect;
CREATE EXTENSION
</code></pre><p>Then we create a test table and insert some data into it:</p>
<pre><code>postgres=# create table testtab (id int);
CREATE TABLE
postgres=# insert into testtab select generate_series(1,2000);
INSERT 0 2000
</code></pre><p>Postgres datafiles are laid out in terms of 8k (by default) pages. To get an visualize what&rsquo;s going on at the page level, we can use a query like this. This is a really simple case because our data is all ints. We can see postgres can store 226 rows per page.</p>
<pre><code>postgres=# with pageinfo as (select * from (select generate_series::int as pagenum from generate_series(0,(select (pg_relation_size/(select setting from pg_settings where name='block_size')::int)-1 from pg_relation_size('testtab')))) pages, heap_page_items(get_raw_page('testtab',pages.pagenum)))
select p1.pagenum, coalesce(p2.c,0) as c from (select distinct pagenum from pageinfo) p1 left join (select pagenum,count(*) as c from pageinfo where lp_len &gt; 0 group by pagenum) p2 on p1.pagenum=p2.pagenum order by p1.pagenum;
 pagenum |  c  
---------+-----
       0 | 226
       1 | 226
       2 | 226
       3 | 226
       4 | 226
       5 | 226
       6 | 226
       7 | 226
       8 | 192
(9 rows)
</code></pre><p>Now what happens if we delete half the rows? We see the number of rows in the pages doesn&rsquo;t change, because even after deletion, the old row versions are still present.</p>
<pre><code>postgres=# delete from testtab where id between 1001 and 2000;
DELETE 1000
postgres=# with pageinfo as (select * from (select generate_series::int as pagenum from generate_series(0,(select (pg_relation_size/(select setting from pg_settings where name='block_size')::int)-1 from pg_relation_size('testtab')))) pages, heap_page_items(get_raw_page('testtab',pages.pagenum)))
select p1.pagenum, coalesce(p2.c,0) as c from (select distinct pagenum from pageinfo) p1 left join (select pagenum,count(*) as c from pageinfo where lp_len &gt; 0 group by pagenum) p2 on p1.pagenum=p2.pagenum order by p1.pagenum;
 pagenum |  c  
---------+-----
       0 | 226
       1 | 226
       2 | 226
       3 | 226
       4 | 226
       5 | 226
       6 | 226
       7 | 226
       8 | 192
(9 rows)
</code></pre><p>But if we vacuum the table, we can see the number of rows goes down. In fact, we&rsquo;ll see the number of pages in the table actually goes down as well. Vacuum is able to completely drop <strong>empty</strong> pages from the end of a table. Note the &ldquo;truncated 9 to 5 pages&rdquo; in the output. It is not able to drop empty pages in the middle, as we&rsquo;ll see soon.</p>
<pre><code>postgres=# vacuum verbose testtab;
INFO:  vacuuming &quot;public.testtab&quot;
INFO:  &quot;testtab&quot;: removed 1000 row versions in 5 pages
INFO:  &quot;testtab&quot;: found 1000 removable, 1000 nonremovable row versions in 9 out of 9 pages
DETAIL:  0 dead row versions cannot be removed yet, oldest xmin: 20635
There were 0 unused item pointers.
Skipped 0 pages due to buffer pins, 0 frozen pages.
0 pages are entirely empty.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
INFO:  &quot;testtab&quot;: truncated 9 to 5 pages
DETAIL:  CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.02 s
VACUUM
postgres=# with pageinfo as (select * from (select generate_series::int as pagenum from generate_series(0,(select (pg_relation_size/(select setting from pg_settings where name='block_size')::int)-1 from pg_relation_size('testtab')))) pages, heap_page_items(get_raw_page('testtab',pages.pagenum)))
select p1.pagenum, coalesce(p2.c,0) as c from (select distinct pagenum from pageinfo) p1 left join (select pagenum,count(*) as c from pageinfo where lp_len &gt; 0 group by pagenum) p2 on p1.pagenum=p2.pagenum order by p1.pagenum;
 pagenum |  c  
---------+-----
       0 | 226
       1 | 226
       2 | 226
       3 | 226
       4 |  96
(5 rows)
</code></pre><p>So now we reinsert our deleted rows, and delete all but the last one. As expected, we&rsquo;re back to 9 pages.</p>
<pre><code>postgres=# insert into testtab select generate_series(1001,2000);
INSERT 0 1000
postgres=# delete from testtab where id between 1001 and 1999;
DELETE 999
postgres=# with pageinfo as (select * from (select generate_series::int as pagenum from generate_series(0,(select (pg_relation_size/(select setting from pg_settings where name='block_size')::int)-1 from pg_relation_size('testtab')))) pages, heap_page_items(get_raw_page('testtab',pages.pagenum)))
select p1.pagenum, coalesce(p2.c,0) as c from (select distinct pagenum from pageinfo) p1 left join (select pagenum,count(*) as c from pageinfo where lp_len &gt; 0 group by pagenum) p2 on p1.pagenum=p2.pagenum order by p1.pagenum;
 pagenum |  c  
---------+-----
       0 | 226
       1 | 226
       2 | 226
       3 | 226
       4 | 226
       5 | 226
       6 | 226
       7 | 226
       8 | 192
(9 rows)
</code></pre><p>And finally we vacuum again. This time we can see that we are left with one half filled page, three empty pages, and a last page with a single row in it. Even though we deleted rows, our table size does not decrease.</p>
<pre><code>postgres=# vacuum verbose testtab;
INFO:  vacuuming &quot;public.testtab&quot;
INFO:  &quot;testtab&quot;: removed 999 row versions in 5 pages
INFO:  &quot;testtab&quot;: found 999 removable, 1001 nonremovable row versions in 9 out of 9 pages
DETAIL:  0 dead row versions cannot be removed yet, oldest xmin: 20638
There were 0 unused item pointers.
Skipped 0 pages due to buffer pins, 0 frozen pages.
0 pages are entirely empty.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
VACUUM
postgres=# with pageinfo as (select * from (select generate_series::int as pagenum from generate_series(0,(select (pg_relation_size/(select setting from pg_settings where name='block_size')::int)-1 from pg_relation_size('testtab')))) pages, heap_page_items(get_raw_page('testtab',pages.pagenum)))
select p1.pagenum, coalesce(p2.c,0) as c from (select distinct pagenum from pageinfo) p1 left join (select pagenum,count(*) as c from pageinfo where lp_len &gt; 0 group by pagenum) p2 on p1.pagenum=p2.pagenum order by p1.pagenum;
 pagenum |  c  
---------+-----
       0 | 226
       1 | 226
       2 | 226
       3 | 226
       4 |  96
       5 |   0
       6 |   0
       7 |   0
       8 |   1
(9 rows) 
</code></pre><p>In a small example like this, this doesn&rsquo;t matter much. But at scale, all those empty pages still have to be checked during operations like sequential scans. They can have a significant performance impact. That empty space can be re-used for new data, so it&rsquo;s not all bad news. Still, this is something every admin or developer working with postgres needs to be cognizant of.</p>
<p>So what are the takeaways here?</p>
<ol>
<li>Vacuum early, vacuum often. The more often you vacuum, the less chance you have for dead rows to stack up. Of course, vacuuming also has a cost, so you can&rsquo;t do it <strong>too</strong> much. But vacuum actually gets more expensive the more rows it needs to remove, so on balance it&rsquo;s better to do it too often than not often enough.</li>
<li>Depending on your usage pattern, it may make sense to have your application code explicitly vacuum after large modification operations. This won&rsquo;t fit every environment well, but taking vacuum into consideration in your workflows may be beneficial, rather than just relying on autovacuum to clean up.</li>
<li>In the case of really large data modifications, it may make sense to completely rebuild the table with VACUUM FULL. This will rewrite all the pages and compact the empty space. However it&rsquo;s also expensive and requires an exclusive lock. Plus it really only makes sense to do if the large-scale modification is an unusual circumstance. If you&rsquo;re just going to create a bunch of empty pages again immediately, there isn&rsquo;t any point.</li>
</ol>

                    </div>
                </section>
                
                <h1 class="content-subhead">21 Nov 2020, 17:55</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://jlucasdba.github.io/posts/firstpost/" class="post-title">Hello, World!</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>So this is an experiment. I&rsquo;ve never tried to do a blog before, and we&rsquo;re going to see how it goes.</p>
<p>I&rsquo;ve worked in tech a lot of years now. Every so often you come across an interesting problem or learn something new, and I&rsquo;ve often thought it would be nice to have a place to write it up and share it with others. So that&rsquo;s what I aim for this page to be. I can&rsquo;t promise I&rsquo;ll update often, or that anyone else will find the content interesting. But if I write it up, I can promise it&rsquo;ll be something <strong>I</strong> found interesting. Maybe you will too.</p>
<p>My primary trade is as a database admin, so you can expect a lot of Postgres, and maybe some Oracle. I also write a lot of Python, so probably some of that as well. And after that we&rsquo;ll see.</p>
<p>Welcome aboard.</p>

                    </div>
                </section>
                
            </div>
            

            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="https://gohugo.io/" target="_blank">hugo</a></li>
            <li>Copyright © 2020 James Lucas</li>
            <li>ALL VIEWS AND OPINIONS ARE MY OWN.<br>THE AUTHOR WILL NOT BE LIABLE FOR DAMAGES OR LOSSES RESULTING FROM<br>USE OR IMPLEMENTATION OF INFORMATION OR TECHNIQUES PUBLISHED ON THIS SITE.</li>
        </ul>
    </div>
</div>
<script src='https://jlucasdba.github.io/js/all.min.js'></script>

        </div>
    </div>
</div>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
